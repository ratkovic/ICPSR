{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratkovic/ICPSR/blob/main/ProblemSet1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNEFhp38_7UA"
      },
      "source": [
        "\n",
        "# Part 1: Building a Model and Globe-tossing\n",
        "\n",
        "## Some of this part's material comes from\n",
        "\n",
        "Statistical Rethinking\n",
        "\n",
        "- <https://learning.oreilly.com/library/view/statistical-rethinking/9781482253481/>\n",
        "\n",
        "Corresponding Code\n",
        "\n",
        "- <https://bookdown.org/content/4857/>\n",
        "\n",
        "\"Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection\"  by Danielle J. Navarro\n",
        "\n",
        "- <https://psyarxiv.com/39q8y/>\n",
        "\n",
        "## Libraries we will need\n",
        "\n",
        "Please note that if a library is not installed, you will need to do so! Note that this may take (quite) a few minutes the first time.\n",
        "\n",
        "Also note that you can download the notebook iteslf an run it locally on your computer.  For details on doing so, consult chatGPT.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKVK6RvH_9q3"
      },
      "outputs": [],
      "source": [
        "packages <- c('tidyverse','mixtools')\n",
        "install.packages(setdiff(packages, rownames(installed.packages())))\n",
        "\n",
        "library(tidyverse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Statistical Golems: The Model\n",
        "\n",
        "\n",
        "Sometimes their unyielding logic reveals implications previously hidden to their designers. These implications can be priceless discoveries. Or they may produce silly and dangerous behavior. Rather than idealized angels of reason, scientific models are powerful clay robots without intent of their own, bumbling along according to the myopic instructions they embody. Like with Rabbi Judah’s golem, the golems of science are wisely regarded with both awe and apprehension. We absolutely have to use them, but doing so always entails some risk.\n",
        "\n",
        "## Bayes, Models, Parameters, and Estimation\n",
        "\n",
        "Bayesian Statistics relies on a fully specified model of the outcome. As we discuss in lecture, this means that Bayesians produce a full, transparent statistical model of the data and how the data will inform inference.  On first glance this may seem unobjetionable, but let's contrast it with standard, frequentist approaches in our field.   \n",
        "\n",
        "Consider, for example, heteroskedasticity in your regression's residuals.  The frequentist might implement ``heteroskedasticity robust standard errors,'' which allow for valid inference on regression coefficients without specifying the heteroskedasticity.  Setting aside the inherent difference in inferential frameworks, the Bayesian would urge the social scientist to model the heteroskedasticity and try to learn something from it.  The frequentist would, instead, say that the heteroskedasticity is simply nuisance and distracting from the question at hand, the regression coefficients.\n",
        "\n",
        "Similar arguments play out when using  ``Cluster Robust Standard Errors'', where Bayesian would turn to a hierarchical linear model.  Both views have value, but I am going to suggest that you rely on frequentist methods for simple questions, i.e. a regression coefficient or ATE/ATT, but use Bayesian methods when looking to model complex phenomena: hierarchical models, panel models, and, ultimately, networks, images, and text.\n",
        "\n",
        "This chain of argument puts model selection at the core of Bayesian statistics, to which we turn next.\n",
        "\n",
        "## Model Selection\n",
        "\n",
        "Consider the posterior density for a given set of parameters $\\theta$ given a model, $\\mathcal M$ and data $\\mathcal D$:\n",
        "\n",
        "- $Pr(\\theta|  D,  M)$\n",
        "\n",
        "How should we choose choose among models?\n",
        "\n",
        "- $Pr( M| D)$\n",
        "\n",
        "i.e. compare $ M_1$ and $ M_2$, i.e. $Pr( M_1| D)$ vs $Pr( M_2| D)$?\n",
        "\n",
        "\n",
        "## General Thoughts\n",
        "\n",
        "This question will be aesthetic as well as mathematical.  There is always Occam's razor, but here's a few more quotes that guide me on assessing models:\n",
        "\n",
        "Dennis Lindley:\n",
        "\n",
        "- If you can’t do simple problems, how can you do complicated ones?\n",
        "\n",
        "George Box:\n",
        "\n",
        "- Since all models are wrong the scientist must be alert to what is importantly wrong. It is  inappropriate to be concerned about mice when there are tigers abroad.\n",
        "\n",
        "Bernardo and Smith:\n",
        "\n",
        "- Many authors . . . highlight a distinction between what one might call scientific and technological approaches to models. The essence of the dichotomy is that scientists are assumed to seek explanatory models, which aim at providing insight into and understanding of the “true” mechanisms of the phenomenon under study; whereas technologists are content with empirical models, which are not concerned with the “truth”, but simply providing a reliably basis for practical action in predicting and controlling phenomena of interest.\n",
        "- See Breiman, ``Statistics: The Two Cultures\"\n",
        "\n",
        "We will return to criteria for selecting across models after practicing sampling from a posterior.\n",
        "\n",
        "## Globe tossing: An Example of Bayesian Updating\n",
        "\n",
        "We begin with a simple example, showing how the data, working through the likelihood, and the prior can combine to generate a posterior density.  Let's imagine we want to estimate the proportion of the earth's surface that is covered with water (correct answer: 2/3 and, yes, the earth is round).  We could do this by randomly selecting points on a globe and calculating the percent that are water.\n",
        "\n",
        "\n",
        "## Building a Model\n",
        "\n",
        "In a Bayesian setting, the components of a model are:\n",
        "\n",
        "- A likelihood function: ``The number of ways each conjecture could produce an observation''\n",
        "\n",
        "- The parameters: ``the accumulated number of ways each conjecture cold produce the entire data''\n",
        "\n",
        "- A prior: ``the initial plausibility of each conjectured cause of the data''\n",
        "\n",
        "\n",
        "The model has two different componenets, one observed and one unobserved.\n",
        "\n",
        "- Observed: likelihood; data given parameters\n",
        "\n",
        "- Unobserved: parameters\n",
        "\n",
        "## Globe tossing: The Model\n",
        "\n",
        "We can write the model for tossing the globe in hierarchical form as\n",
        "\n",
        "$w|p,n \\sim Binomial(n,p)$\n",
        "\n",
        "$p \\sim Unif(0,1)$\n",
        "\n",
        "where $n=l+w$.\n",
        "\n",
        "The posterior can be written as\n",
        "- $Pr(p|w,l)=\\frac{Pr(w,l|p)Pr(p)}{Pr(w,l)}$\n",
        "\n",
        "where\n",
        "\n",
        "- Posterior = (Probability  of  data $\\times$ Prior)/(Average Probability of the Data)\n",
        "\n",
        "Note that since the denominator is not a function of the parameters, we get the result that the posterior is proportional to the data density times the prior.\n",
        "\n",
        "## The vector of data\n",
        "\n",
        "\n",
        "We could model this with the vector $d0$ below, with outcomes for water and land.  Say we conduct this experiment and recover 6 w's and 3 l's,\n",
        "\n"
      ],
      "metadata": {
        "id": "eo1Rxise7Nqk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzdtxZ_H_xfU"
      },
      "outputs": [],
      "source": [
        "d0 <- tibble(toss = c(\"w\", \"l\", \"w\", \"w\", \"w\", \"l\", \"w\", \"l\", \"w\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to see how, as we go from no data to nine observations, how the posterior distribtuion evolves.\n",
        "\n",
        "## Globe tossing"
      ],
      "metadata": {
        "id": "fps26R1q70VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  d <-\n",
        "  d0 %>%\n",
        "  mutate(n_trials  = 1:9,\n",
        "         n_success = cumsum(toss == \"w\"))\n",
        "\n",
        "  d"
      ],
      "metadata": {
        "id": "vjkjQZR174J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Globe tossing\n",
        "\n",
        "Now, we are going to generate a plot showing how each new datum moves the posterior. In each square, the dashed line is the _previous_ posterior estimate, and the solid line is the next one.\n"
      ],
      "metadata": {
        "id": "YAnw3pQD7-fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length <- 50\n",
        "\n",
        "d %>%\n",
        "  expand(nesting(n_trials, toss, n_success),\n",
        "         p_water = seq(from = 0, to = 1, length.out = sequence_length)) %>%\n",
        "  group_by(p_water) %>%\n",
        " # you can learn more about lagging here: https://www.rdocumentation.org/packages/stats/versions/3.5.1/topics/lag or here: https://dplyr.tidyverse.org/reference/lead-lag.html\n",
        "  mutate(lagged_n_trials  = lag(n_trials, n = 1),\n",
        "         lagged_n_success = lag(n_success, n = 1)) %>%\n",
        "  ungroup() %>%\n",
        "  mutate(prior      = ifelse(n_trials == 1, .5,\n",
        "                             dbinom(x    = lagged_n_success,\n",
        "                                    size = lagged_n_trials,\n",
        "                                    prob = p_water)),\n",
        "         likelihood = dbinom(x    = n_success,\n",
        "                             size = n_trials,\n",
        "                             prob = p_water),\n",
        "         strip      = str_c(\"n = \", n_trials)) %>%\n",
        "  # the next three lines allow us to normalize the prior and the likelihood,\n",
        "  # putting them both in a probability metric\n",
        "  group_by(n_trials) %>%\n",
        "  mutate(prior      = prior / sum(prior),\n",
        "         likelihood = likelihood / sum(likelihood)) %>%\n",
        "\n",
        "  # plot!\n",
        "  ggplot(aes(x = p_water)) +\n",
        "  geom_line(aes(y = prior),\n",
        "            linetype = 2) +\n",
        "  geom_line(aes(y = likelihood)) +\n",
        "  scale_x_continuous(\"proportion water\", breaks = c(0, .5, 1)) +\n",
        "  scale_y_continuous(\"plausibility\", breaks = NULL) +\n",
        "  theme(panel.grid = element_blank()) +\n",
        "  facet_wrap(~strip, scales = \"free_y\")"
      ],
      "metadata": {
        "id": "IibENpZM7_j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To read this figure, the top left shows how the posterior density goes from flat (the prior) to linear with the introduction of the first datum, a $w$.  The next panel in the figure shows how this line evolves to a parabolic shape with two pieces of data.  By $n=9$, in the lower right, the posterior seems sensible and peaking near $2/3$.\n",
        "\n",
        "## Motors\n",
        "\n",
        "All of our inference is going to come from analyzing this posterior. We could find the maximum a posterior (MAP) estimate by looking at the topmost point in the bottom right.  We could ask, for example, the probability that the proportion of water is above 1/2 by looking at the corresponding region of the posterior.\n",
        "\n",
        "In more complex settings, we cannot picture such a simple density. Instead, we may sample from the posterior and analyze these samples.  \n",
        "\n",
        "We are going to use a grid sampler to sample from the posterior of the grid example, sampling at a set of discrete points where we have evaluated the posterior and sampling each point proportional to this value.\n",
        "\n",
        "In the next question, we deal with some more advanced methods.\n",
        "\n",
        "## Grid Sampling: An Example"
      ],
      "metadata": {
        "id": "bgGMHCbFAzU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  d <-\n",
        "    tibble(p_grid = seq(from = 0, to = 1, length.out = 20),      # define grid\n",
        "           prior  = 1) %>%                                       # define prior\n",
        "    mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%  # compute likelihood at each value in grid\n",
        "    mutate(unstd_posterior = likelihood * prior) %>%             # compute product of likelihood and prior\n",
        "    mutate(posterior = unstd_posterior / sum(unstd_posterior))   # standardize the posterior, so it sums to 1"
      ],
      "metadata": {
        "id": "8ADkb7hfA250"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Grid Points and Accuracy\n",
        "\n",
        "Next, we see ho allowing for the evaluation of more grid points, we get a better estimate of the posterior. The problem, of course, is that sampling more points is computationally expensive--a central concern in these models."
      ],
      "metadata": {
        "id": "maPbFe3pA-0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p2 <-\n",
        "  tibble(p_grid = seq(from = 0, to = 1, length.out = 5),\n",
        "         prior  = 1) %>%\n",
        "  mutate(likelihood = dbinom(6, size = 9, prob = p_grid)) %>%\n",
        "  mutate(unstd_posterior = likelihood * prior) %>%\n",
        "  mutate(posterior = unstd_posterior / sum(unstd_posterior)) %>%\n",
        "\n",
        "  ggplot(aes(x = p_grid, y = posterior)) +\n",
        "  geom_point() +\n",
        "  geom_line() +\n",
        "  labs(subtitle = \"5 points\",\n",
        "       x = \"probability of water\",\n",
        "       y = \"posterior probability\") +\n",
        "  theme(panel.grid = element_blank())\n",
        "\n",
        "p2"
      ],
      "metadata": {
        "id": "I-LagizOBBMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Points and Accuracy\n"
      ],
      "metadata": {
        "id": "Lzz6QPh1BCVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 <-\n",
        "  d %>%\n",
        "  ggplot(aes(x = p_grid, y = posterior)) +\n",
        "  geom_point() +\n",
        "  geom_line() +\n",
        "  labs(subtitle = \"20 points\",\n",
        "       x = \"probability of water\",\n",
        "       y = \"posterior probability\") +\n",
        "  theme(panel.grid = element_blank())\n",
        "\n",
        "p1"
      ],
      "metadata": {
        "id": "IATcTsI5BD8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Sampling Regression Coefficients Using Gibbs, MH, and HMC\n",
        "\n",
        "The posterior from the previous example is not useful in practice: the model is too simple, and the posterior is one-dimensional. We will never really encounter this in practice in Bayesian statisitics. Next, we turn to a slightly more complex setting, the regression model with known error variance.\n",
        "\n",
        "## This part's material comes from\n",
        "\n",
        "\"MCMC using Hamiltonian dynamics\" by Radford Neal\n",
        "\n",
        "- <https://arxiv.org/abs/1206.1901>\n",
        "\n",
        "## Libraries we will need\n",
        "\n",
        "```{r, echo = TRUE, message=FALSE, warning=FALSE}\n",
        "library(mixtools)\n",
        "library(MASS)\n",
        "```\n",
        "\n",
        "## Bayesian Regression\n",
        "\n",
        "Consider a simple model $y_i=X_i^\\top \\beta+ e_i;\\;\\; e_i \\sim  N(0,1)$\n",
        "\n",
        "where $\\beta=[2,3]^\\top$.\n",
        "\n",
        "\\medskip\n",
        "\n",
        "We are going to put flat priors on $\\beta$ and assume the error variance is known to be $1$, i.e.\n",
        "\\begin{align*}\n",
        "y_i|X_i, \\beta & \\sim  N(X_i^\\top \\beta,1)\\\\\n",
        "\\Pr(\\beta_j) & \\propto 1\n",
        "\\end{align*}\n",
        "\n",
        "Then, we know\n",
        "\\begin{align*}\n",
        "\\beta|\\cdot &\\sim  N\\left((X^\\top X)^{-1}X^\\top y, (X^\\top X)^{-1}\\right)\n",
        "\\end{align*}\n",
        "\n",
        "## Methods for Sampling from the Posterior\n",
        "\n",
        "We are going to consider the following samplers for drawing samples of $\\beta$:\n",
        "\n",
        "-- Joint Gibbs sampler (Gibbs2)\n",
        "\n",
        "-- Coordinatewise Gibbs (Gibbs1)\n",
        "\n",
        "--  Metropolis-Hastings (MH)\n",
        "\n",
        "-- Hamiltonian Monte Carlo (HMC)\n",
        "\n",
        "## Methods for Assessing the Chains\n",
        "\n",
        "-- Computational speed\n",
        "\n",
        "-- Mixing and Convergence\n",
        "\n",
        "+ Mixing: How quickly does the chain get to its steady state?\n",
        "\n",
        "+ Convergence: Do multiple chains get to the same place? (Geweke statistic/trace plot)\n",
        "\n",
        "--  Auto-correlation\n",
        "\n",
        "+ Does the chain explore the posterior efficiently?\n",
        "\n",
        "##  Drawing the data\n",
        "First, let's draw the data:\n"
      ],
      "metadata": {
        "id": "Y80K-oKOBF97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set.seed(1)\n",
        "n <- 10\n",
        "p <- 2\n",
        "M <- diag(2)\n",
        "M[M == 0] <- 2 ^ -.5\n",
        "X <- MASS::mvrnorm(n, c(0, 0), Sigma = M)\n",
        "X <- apply(X, 2, scale)\n",
        "y <- X %*% c(2, 3) + rnorm(n)\n",
        "y <- y - mean(y)\n",
        "\n",
        "\n",
        "theta.map <- lm(y ~ X)$coef[-1]\n",
        "\n",
        "theta.map"
      ],
      "metadata": {
        "id": "qjU28WH0BKzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for MH and HMC\n",
        "\n",
        "When evaluating the MH and HMC, we will need to evaluate the posterior under the previous and proposed values. Note these are the _negative_ log posterior and log gradient, which is physicists' convention!"
      ],
      "metadata": {
        "id": "qUQU3YcmBNRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logpost <- function(theta) {\n",
        "  sum((y - X %*% theta) ^ 2) / 2\n",
        "}\n",
        "\n",
        "gradpost <- function(theta) {\n",
        "  -as.vector(t(X) %*% (y - X %*% theta))\n",
        "}"
      ],
      "metadata": {
        "id": "vzG0iK9pBRXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "Let's load some functions that we will use.\n",
        "For now, you can just run this next cell and load the functions. We will revisit them a bit later."
      ],
      "metadata": {
        "id": "OyGub44lBToL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HMC <- function (U, grad_U, epsilon, L, current_q)\n",
        "{\n",
        "  q = current_q\n",
        "  p = rnorm(length(q), 0, 1) # independent standard normal variates\n",
        "  current_p = p\n",
        "  # Make a half step for momentum at the beginning\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "  # Alternate full steps for position and momentum\n",
        "  for (i in 1:L)\n",
        "  {\n",
        "    # Make a full step for the position\n",
        "    q = q + epsilon * p\n",
        "    # Make a full step for the momentum, except at end of trajectory\n",
        "    if (i != L)\n",
        "      p = p - epsilon * grad_U(q)\n",
        "  }\n",
        "  # Make a half step for momentum at the end.\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "  # Negate momentum at end of trajectory to make the proposal symmetric\n",
        "  p = -p\n",
        "  # Evaluate potential and kinetic energies at start and end of trajectory\n",
        "  current_U = U(current_q)\n",
        "  current_K = sum(current_p ^ 2) / 2\n",
        "  proposed_U = U(q)\n",
        "  proposed_K = sum(p ^ 2) / 2\n",
        "  # Accept or reject the state at end of trajectory, returning either\n",
        "  # the position at the end of the trajectory or the initial position\n",
        "  if (runif(1) < exp(current_U - proposed_U + current_K - proposed_K))\n",
        "  {\n",
        "    return (q) # accept\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    return (current_q) # reject\n",
        "  }\n",
        "}\n",
        "\n",
        "HMC_NUTS <- function (U, grad_U, epsilon, L, current_q)\n",
        "{\n",
        "  q = current_q\n",
        "  p = rnorm(length(q), 0, 1) # independent standard normal variates\n",
        "  current_p = p\n",
        "  # Make a half step for momentum at the beginning\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "  q.run <- q\n",
        "  # Alternate full steps for position and momentum\n",
        "  for (i in 1:L)\n",
        "  {\n",
        "    # Make a full step for the position\n",
        "    q = q + epsilon * p\n",
        "    q.run <- cbind(q.run, q)\n",
        "    # Make a full step for the momentum, except at end of trajectory\n",
        "    if (i != L)\n",
        "      p = p - epsilon * grad_U(q)\n",
        "  }\n",
        "  # Make a half step for momentum at the end.\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "\n",
        "  q.run\n",
        "}\n",
        "\n",
        "MH <- function (U, epsilon, current_q)\n",
        "{\n",
        "  q = current_q\n",
        "  q = current_q + epsilon * rnorm(length(q))\n",
        "  # Evaluate potential and kinetic energies at start and end of trajectory\n",
        "  current_U = U(current_q)\n",
        "  current_K = 0#sum(current_p^2) / 2\n",
        "  proposed_U = U(q)\n",
        "  proposed_K = 0#sum(p^2) / 2\n",
        "  # Accept or reject the state at end of trajectory, returning either\n",
        "  # the position at the end of the trajectory or the initial position\n",
        "  if (runif(1) < exp(current_U - proposed_U + current_K - proposed_K))\n",
        "  {\n",
        "    return (q) # accept\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    return (current_q) # reject\n",
        "  }\n",
        "}\n",
        "\n",
        "# Log posterior at (0,0)\n",
        "logpost(c(0, 0))\n",
        "\n",
        "# Log posterior after 10 steps from (0,0)\n",
        "logpost(HMC(logpost, gradpost, .01, 10, c(0, 0)))"
      ],
      "metadata": {
        "id": "Icn9UMKpBY5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMC Code\n",
        "\n",
        "The function 'HMC' takes a set of inputs and returns a new proposal where\n",
        "\n",
        "- $U$ = the negative log posterior (a function)\n",
        "- $grad\\_U$ = the gradient of $U$ (a function)\n",
        "- $epsilon$ = stepsize\n",
        "- $L$ = number of steps\n",
        "- $current\\_q$ = the current parameter values\n",
        "\n",
        "Below is the code:"
      ],
      "metadata": {
        "id": "bH5aa16FJ-A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HMC <- function (U, grad_U, epsilon, L, current_q) {\n",
        "  q = current_q\n",
        "  p = rnorm(length(q), 0, 1) # independent standard normal variates\n",
        "  current_p = p\n",
        "  # Make a half step for momentum at the beginning\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "  # Alternate full steps for position and momentum\n",
        "  for (i in 1:L) {\n",
        "    # Make a full step for the position\n",
        "    q = q + epsilon * p\n",
        "    # Make a full step for the momentum, except at end of trajectory\n",
        "    if (i != L)\n",
        "      p = p - epsilon * grad_U(q)\n",
        "  }\n",
        "  # Make a half step for momentum at the end.\n",
        "  p = p - epsilon * grad_U(q) / 2\n",
        "  # Negate momentum at end of trajectory to make the proposal symmetric\n",
        "  p = -p\n",
        "  # Evaluate potential and kinetic energies at start and end of trajectory\n",
        "  current_U = U(current_q)\n",
        "  current_K = sum(current_p ^ 2) / 2\n",
        "  proposed_U = U(q)\n",
        "  proposed_K = sum(p ^ 2) / 2\n",
        "  # Accept or reject the state at end of trajectory, returning either\n",
        "  # the position at the end of the trajectory or the initial position\n",
        "  if (runif(1) < exp(current_U - proposed_U + current_K - proposed_K))\n",
        "  {\n",
        "    return (q) # accept\n",
        "  } else{\n",
        "    return (current_q) # reject\n",
        "  }\n",
        "}\n"
      ],
      "metadata": {
        "id": "5BAZlvyvKBIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jYlWpxWoKDhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## MH Code\n",
        "\n",
        "The function generating a sample using MH is similar to that from HMC, but a bit simpler as there is no gradient being used.  The arguments are the same as in HMC.\n",
        "\n",
        "- $U$ = the negative log posterior (a function)\n",
        "- $epsilon$ = proposal variance\n",
        "- $current\\_q$ = the current parameter values"
      ],
      "metadata": {
        "id": "gjyCbO4tKDkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MH <- function (U, epsilon, current_q)\n",
        "{\n",
        "  q = current_q\n",
        "  q = current_q + epsilon*rnorm(length(q))\n",
        "  # Evaluate potential and kinetic energies at start and end of trajectory\n",
        "  current_U = U(current_q)\n",
        "  current_K = 0#sum(current_p^2) / 2\n",
        "  proposed_U = U(q)\n",
        "  proposed_K = 0#sum(p^2) / 2\n",
        "  # Accept or reject the state at end of trajectory, returning either\n",
        "  # the position at the end of the trajectory or the initial position\n",
        "  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))\n",
        "  {\n",
        "    return (q) # accept\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    return (current_q) # reject\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "SS6sPfeyKEpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Methods\n",
        "\n",
        "Let's first initialize containers that will contain all of the output. This will be a list with each element a matrix with the number of rows equal to the number of parameters and number of columns equal to the number of samples.\n",
        "\n",
        "We will start each sampler at the same point $\\theta_0=c(0,0)$.\n"
      ],
      "metadata": {
        "id": "IyeUlUIHKIOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.init <- theta.curr <- c(0, 0)\n",
        "theta.all <- NULL\n",
        "theta.all$Gibbs1 <-\n",
        "  theta.all$Gibbs2 <-\n",
        "  theta.all$MH <-\n",
        "  theta.all$HMC <-\n",
        "  matrix(NA, nr = 2, nc = 2000)"
      ],
      "metadata": {
        "id": "b1qPf6VVKUCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Fitting Gibbs 2\n",
        "\n",
        "We first sample from $Gibbs2$, the model where we update all of the coefficients at once.  We do this $1000$ times.  \n",
        "\n",
        "Please note that, for the problem set, you will need to turn this into a loop, which you can model off of the Gibbs1 approach below.\n",
        "\n",
        "For now, though, we can simply sample $\\beta$ through a single call."
      ],
      "metadata": {
        "id": "EKBhvhY6KWS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.all$Gibbs2 <-\n",
        "  t(MASS::mvrnorm(1000, mu = theta.map, Sigma = solve(crossprod(X))))"
      ],
      "metadata": {
        "id": "4S5T0OSLKiYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting Gibbs 1\n",
        "\n",
        "We next sample from $Gibbs1$, where we sample the coefficients one at a time.  We do it $1000$ times."
      ],
      "metadata": {
        "id": "QpVbEiWmKtSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.curr <- theta.init\n",
        "system.time({\n",
        "  for (i in 1:1000) {\n",
        "    set.seed(i)\n",
        "    theta.all$Gibbs1[, i] <- theta.curr\n",
        "    theta.curr[1] <- rnorm(1,\n",
        "                           mean = lm(y - X[, 2] * theta.curr[2] ~ X[, 1])$coef[2],\n",
        "                           sd = 1 / sum(X[, 1] ^ 2) ^ .5)\n",
        "    theta.curr[2] <- rnorm(1,\n",
        "                           mean = lm(y - X[, 1] * theta.curr[1] ~ X[, 2])$coef[2],\n",
        "                           sd = 1 / sum(X[, 2] ^ 2) ^ .5)\n",
        "  }\n",
        "})"
      ],
      "metadata": {
        "id": "sH8vqxO-KwW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Fit HMC\n",
        "\n",
        "Let's next go ahead and sample using HMC.  Note that we are taking the step size (0.45) and number of steps (10) as fixed, but this of course needs to be estimated in practice.\n"
      ],
      "metadata": {
        "id": "ysjsBcJZK8u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.curr <- theta.init\n",
        "system.time({\n",
        "  for (i in 1:200) {\n",
        "    set.seed(i)\n",
        "    theta.all$HMC[, i] <- theta.curr\n",
        "    theta.curr <- HMC(logpost, gradpost, .45, 10, theta.curr)\n",
        "  }\n",
        "})\n",
        "\n",
        "# Where do we start? Where do we end up?\n",
        "theta.init\n",
        "theta.curr"
      ],
      "metadata": {
        "id": "2vtVV8YQK_Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Fit Metropolis-Hastings\n",
        "\n",
        "Next, the simple MH random walk algorithm, with $0.25 I$ as the proposal (and noting of course that this must be tuned in practice too!)."
      ],
      "metadata": {
        "id": "m3sz9xKMLIsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.curr <- theta.init\n",
        "system.time({\n",
        "  for (i in 1:1200) {\n",
        "    set.seed(i)\n",
        "    theta.all$MH[, i] <- theta.curr\n",
        "    theta.curr <- MH(logpost, .25, theta.curr)\n",
        "  }\n",
        "})\n",
        "theta.init\n",
        "theta.curr"
      ],
      "metadata": {
        "id": "uTGtde2DLKj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the posterior: Joint Gibbs\n",
        "In the next figures, the ellipses are level sets of the true posterior, and the lines show how the estimates explore the space. Ideally, we would see the method concentrate in a high posterior setting and move at random, with no signs of autocorrelation.  Joint Gibbs, again, is the best answer, to be compared against the others."
      ],
      "metadata": {
        "id": "ars1tmVFLNBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\n",
        "  theta.all$Gibbs2[1, ],\n",
        "  theta.all$Gibbs2[2, ],\n",
        "  type = \"l\",\n",
        "  xlim = c(-1, 3.5),\n",
        "  ylim = c(-.1, 4.5)\n",
        ")\n",
        "points(\n",
        "  theta.map[1],\n",
        "  theta.map[2],\n",
        "  pch = 19,\n",
        "  col = \"red\",\n",
        "  cex = 2\n",
        ")\n",
        "\n",
        "for (i.alpha in 1:6)\n",
        "  mixtools::ellipse(theta.map,\n",
        "                    solve(crossprod(X)),\n",
        "                    alpha = 10 ^ (-i.alpha),\n",
        "                    col = gray(.5))"
      ],
      "metadata": {
        "id": "k8mXech-LPtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the posterior: Coordinatewise Gibbs\n"
      ],
      "metadata": {
        "id": "XuUkDfpJLdl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\n",
        "  theta.all$Gibbs1[1, ],\n",
        "  theta.all$Gibbs1[2, ],\n",
        "  type = \"l\",\n",
        "  xlim = c(-1, 3.5),\n",
        "  ylim = c(-.1, 4.5)\n",
        ")\n",
        "points(\n",
        "  theta.map[1],\n",
        "  theta.map[2],\n",
        "  pch = 19,\n",
        "  col = \"red\",\n",
        "  cex = 2\n",
        ")\n",
        "\n",
        "for (i.alpha in 1:6)\n",
        "  mixtools::ellipse(theta.map,\n",
        "                    solve(crossprod(X)),\n",
        "                    alpha = 10 ^ (-i.alpha),\n",
        "                    col = gray(.5))"
      ],
      "metadata": {
        "id": "V_2BIp70Lghy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the posterior: Metropolis-Hastings\n"
      ],
      "metadata": {
        "id": "Zkn0iJKhL0Rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\n",
        "  theta.all$MH[1, ],\n",
        "  theta.all$MH[2, ],\n",
        "  type = \"l\",\n",
        "  xlim = c(-1, 3.5),\n",
        "  ylim = c(-.1, 4.5)\n",
        ")\n",
        "points(\n",
        "  theta.map[1],\n",
        "  theta.map[2],\n",
        "  pch = 19,\n",
        "  col = \"red\",\n",
        "  cex = 2\n",
        ")\n",
        "\n",
        "for (i.alpha in 1:6)\n",
        "  mixtools::ellipse(theta.map,\n",
        "                    solve(crossprod(X)),\n",
        "                    alpha = 10 ^ (-i.alpha),\n",
        "                    col = gray(.5))"
      ],
      "metadata": {
        "id": "ClLy59E8L48l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the posterior: Hamiltonian Monte Carlo\n"
      ],
      "metadata": {
        "id": "OmyTi2-hL8cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot(\n",
        "  theta.all$HMC[1, ],\n",
        "  theta.all$HMC[2, ],\n",
        "  type = \"l\",\n",
        "  xlim = c(-1, 3.5),\n",
        "  ylim = c(-.1, 4.5)\n",
        ")\n",
        "points(\n",
        "  theta.map[1],\n",
        "  theta.map[2],\n",
        "  pch = 19,\n",
        "  col = \"red\",\n",
        "  cex = 2\n",
        ")\n",
        "\n",
        "for (i.alpha in 1:6)\n",
        "  mixtools::ellipse(theta.map,\n",
        "                    solve(crossprod(X)),\n",
        "                    alpha = 10 ^ (-i.alpha),\n",
        "                    col = gray(.5))"
      ],
      "metadata": {
        "id": "DKdi3r9gL9-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Why NUTS?\n",
        "\n",
        "STAN implements what it calls the NUTS or No U-Turn Sampler.  The reason for this is that the HMC proposal will, if you take a long enough path, exhibit cyclic behavior:  "
      ],
      "metadata": {
        "id": "3hrCWJI0MCIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nuts1<-HMC_NUTS(logpost,gradpost,.01,1000,c(0,0))\n",
        "plot(nuts1[1,],type=\"l\",xlab=\"L\",ylab=\"beta1\")"
      ],
      "metadata": {
        "id": "e1USEAbnMEeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NUTS sampler keeps taking gradient steps for a proposal but stops before making a U-Turn.\n",
        "\n",
        "## Traceplot and autocorrelation\n",
        "\n",
        "The goal is to explore the full posterior, where we spend more time in high-density regions of the full posterior.  We don't want to get stuck in certain areas, meaning we want a low auto-correlation (this can be measured in the effective sample size statistic).  Lastly, to reduce auto-correlation and memory requirements, we may save every 10th or 100th sample, a process known as thinning.\n",
        "\n",
        "We next look at traceplots for each method, where we want to see no systematic behavior.  We will use this simple function to measure autocorrelation."
      ],
      "metadata": {
        "id": "23q13LdzMJdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ac<-function(x) {\n",
        "  x<-x[!is.na(x)]\n",
        "  cor(x[-1],x[-length(x)])\n",
        "}"
      ],
      "metadata": {
        "id": "-vQS7qR7MLln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traceplots"
      ],
      "metadata": {
        "id": "TyBy-wbJMRm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Joint Gibbs\n",
        "plot(theta.all$Gibbs2[1, !is.na(theta.all$Gibbs2[1, ])],\n",
        "     type = \"l\",\n",
        "     xlab = \"Iteration\",\n",
        "     ylab = \"beta1\",\n",
        "     main = \"Joint Gibbs\")\n",
        "\n",
        "ac(theta.all$Gibbs2[1, ])\n",
        "\n",
        "# Coordinatewise Gibbs\n",
        "plot(theta.all$Gibbs1[1, !is.na(theta.all$Gibbs1[1, ])],\n",
        "     type = \"l\",\n",
        "     xlab = \"Iteration\",\n",
        "     ylab = \"beta1\",\n",
        "     main = \"Coordinatewise Gibbs\")\n",
        "\n",
        "ac(theta.all$Gibbs1[1, ])\n",
        "\n",
        "# MH\n",
        "plot(theta.all$MH[1, !is.na(theta.all$MH[1, ])],\n",
        "     type = \"l\",\n",
        "     xlab = \"Iteration\",\n",
        "     ylab = \"beta1\",\n",
        "     main = \"Metropolis-Hastings\")\n",
        "\n",
        "ac(theta.all$MH[1, ])\n",
        "\n",
        "# HMC\n",
        "plot(theta.all$HMC[1, !is.na(theta.all$HMC[1, ])],\n",
        "     type = \"l\",\n",
        "     xlab = \"Iteration\",\n",
        "     ylab = \"beta1\",\n",
        "     main = \"Hamiltonian Monte Carlo\")\n",
        "\n",
        "ac(theta.all$HMC[1, ])"
      ],
      "metadata": {
        "id": "asWx55N6MUqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convergence\n",
        "\n",
        "Another method for checking convergence is to run multiple chains and making sure they mix well:"
      ],
      "metadata": {
        "id": "D39lkPrhM5ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta.all$HMC2 <- theta.all$HMC3 <- theta.all$HMC * NA\n",
        "theta.curr <- c(5, 5)\n",
        "for (i in 1:200) {\n",
        "  set.seed(i)\n",
        "  theta.all$HMC2[, i] <- theta.curr\n",
        "  theta.curr <- HMC(logpost, gradpost, .1, 10, theta.curr)\n",
        "}\n",
        "\n",
        "theta.curr <- c(5, -5)\n",
        "for (i in 1:200) {\n",
        "  set.seed(i)\n",
        "  theta.all$HMC3[, i] <- theta.curr\n",
        "  theta.curr <- HMC(logpost, gradpost, .1, 10, theta.curr)\n",
        "}"
      ],
      "metadata": {
        "id": "uN7VCo_0M7mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various statistics for assessing the convergence through running multiple chains.  I recommend just plotting them overlaid, as above, but the Geweke statistic is effectively a t-statistic to see if the chains differ, and that can be reported as well.\n",
        "\n",
        "# Questions\n",
        "\n",
        "We repeat the regression comparisons in the second part, but instead assume that the error variance is not known to be $1$ and must instead be estimated from the sampler.  You can assume a Jeffreys Prior $\\Pr(\\sigma^2) \\propto 1/\\sigma^2$ over $\\sigma^2$.\n",
        "\n",
        "1. Calculate the appropriate conditional posterior densities.  $\\beta$ is done, though you will have to insert a  $\\sigma^2$ as appropriate.\n",
        "2. Update the functions 'logpost' and 'gradpost', allowing now for three parameters instead of two, due to the addition of $\\sigma^2$. Note that the Gibbs2 function will now need to be done in a loop, since you will satisfy $\\beta|\\sigma^2$ and $\\sigma^2|\\beta$ in turn.  Note for the MH and HMC algorithms: ensure that the value of $\\sigma^2$ is never negative!  How can you do this? (Extra: There's the obvious way of just truncation, so rejecting any value where $\\sigma^2<0$, but what else could be done?  This is where change-of-variables could help!)\n",
        "3. Update the relevant figures. You only need to plot the regression coefficients in the figure using ellipses.  For the traceplots, include $\\sigma^2$ in a different figure.\n",
        "4. How do existing algorithms handle estimating the tuning parameters such as the proposal variance for $MH$ and the step size and epsilon in $HMC$?   How sensitve is $MH$ and $HMC$ to epsilon and (for HMC) L?\n",
        "5. (*) (<-- The star means this next question is challenging.  Do what you can!)  Recreate the HMC and MH samplers for a logistic regression.  You may want to change the coefficients from $(2,3)$ to, maybe, $(.2, .3)$, so the observed values aren't all 0's or 1's.  Note also that the ellipses of the true coefficients will need to change.\n",
        "6. (*) Use the functions in the R package `brms` to replicate the traceplot and coefficient plots.  How does the `STAN` output look compared to what we coded up?"
      ],
      "metadata": {
        "id": "bGWEVBEcM-wR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlFktIBcRBikJKILH2DPTT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}